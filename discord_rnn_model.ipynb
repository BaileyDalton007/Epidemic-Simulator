{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "discord_rnn_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dU_LsF2ijpTSQPsqpTXjJKzi2c4inbCy",
      "authorship_tag": "ABX9TyPNKzMV0wKEcKmL+0MDLJ+Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaileyDalton007/Epidemic-Simulator/blob/main/discord_rnn_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "j9cdsT_4KwqZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import csv\n",
        "from os import linesep\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec as w2v"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uploading files**"
      ],
      "metadata": {
        "id": "rYddoPQwK5iZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DxDBXPuYJUFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to whatever path you have your files in\n",
        "DATA_PATH = '/content/drive/MyDrive/discord_rnn_data/'"
      ],
      "metadata": {
        "id": "QHDGADsNKqgs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use training files from drive\n",
        "import os\n",
        "\n",
        "file_names = os.listdir(DATA_PATH + 'training_data/')"
      ],
      "metadata": {
        "id": "GND9pjdrMELS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Preprocessing**"
      ],
      "metadata": {
        "id": "b-EuHLBBMJxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prime max length seems to be around 15-20, see readme for graph\n",
        "MAX_MSG_LENGTH = 20"
      ],
      "metadata": {
        "id": "hgYzSqYSamOQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create master array for messages from all files\n",
        "msg_array = []\n",
        "\n",
        "for file in file_names:\n",
        "  file_path = DATA_PATH + 'training_data/' + file\n",
        "  with open(file_path, 'r', encoding='utf8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "\n",
        "    for row in csv_reader:\n",
        "      msg_array.append(row[0].lower())\n",
        "\n",
        "# Tokenize msg_array\n",
        "msg_array = [sub.split() for sub in msg_array]\n",
        "\n",
        "# Removes empty lists that get through somehow\n",
        "msg_array = [ele for ele in msg_array if ele != []]\n",
        "\n",
        "# Makes sure every message is the same length\n",
        "for i, msg in enumerate(msg_array):\n",
        "  msg_array[i] = msg[:MAX_MSG_LENGTH]"
      ],
      "metadata": {
        "id": "oZL96SbAVwfu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training / Loading Word2Vec Model**"
      ],
      "metadata": {
        "id": "eVmmW9lESS8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VECTOR_SIZE = 100"
      ],
      "metadata": {
        "id": "ohVjoTueTd5-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load w2v model from drive\n",
        "word_model = w2v.load(f'{DATA_PATH}word_models/word2vec_{VECTOR_SIZE}.model')"
      ],
      "metadata": {
        "id": "d0PmomVLKcHu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the w2v model\n",
        "word_model = w2v(msg_array, size=VECTOR_SIZE, min_count=2, window=5, iter=100)"
      ],
      "metadata": {
        "id": "TDa9ieIVSAHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving w2v model to drive\n",
        "word_model.save(f'{DATA_PATH}word_models/word2vec_{VECTOR_SIZE}.model')"
      ],
      "metadata": {
        "id": "6BEbE-kFTFYz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some variables and functions to interact with the w2v model\n",
        "pretrained_weights = word_model.wv.vectors\n",
        "vocab_size, embedding_size = pretrained_weights.shape\n",
        "\n",
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]"
      ],
      "metadata": {
        "id": "5F797ZcCScJG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing Data for LSTM model** "
      ],
      "metadata": {
        "id": "pOAW1I-gaEAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates empty numpy arrays\n",
        "train_x = np.zeros([len(msg_array), MAX_MSG_LENGTH], dtype=np.int32)\n",
        "train_y = np.zeros([len(msg_array)], dtype=np.int32)\n",
        "# Fills arrays, each row a message and each column a word\n",
        "for i, msg in enumerate(msg_array):\n",
        "  for t, word in enumerate(msg[:-1]):\n",
        "    train_x[i, t] = word2idx(word)\n",
        "  \n",
        "  train_y[i] = word2idx(msg[-1])"
      ],
      "metadata": {
        "id": "hvwTWqbXaIvd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the model**"
      ],
      "metadata": {
        "id": "pPsT4stqUxMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "DndjbzbiN6AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Embedding layer for w2v model\n",
        "model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights]))\n",
        "\n",
        "model.add(tf.keras.layers.LSTM(units=embedding_size))\n",
        "model.add(tf.keras.layers.Dense(units=vocab_size))\n",
        "model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PyHtC0RxWp42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions to interact with the model\n",
        "# From https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b\n",
        "\n",
        "\n",
        "# Model outputs a vector of probabilites of the next word (preds), and this\n",
        "# function will take that and choose the word generated based of the sampling\n",
        "# described here: \n",
        "# https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "  if temperature <= 0:\n",
        "    return np.argmax(preds)\n",
        "\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "# Generates next num_generated words from the text\n",
        "def generate_next(text, num_generated=10, return_probs=False):\n",
        "  word_idxs = [word2idx(word) for word in text.lower().split()]\n",
        "  word_probs = []\n",
        "\n",
        "  for i in range(num_generated):\n",
        "    x = np.zeros([1, 20])\n",
        "    for i, idx in enumerate(word_idxs):\n",
        "      x[0, i] = idx\n",
        "\n",
        "    prediction = model.predict(x)\n",
        "\n",
        "    if return_probs:\n",
        "      word_probs.append(max(prediction[0]))\n",
        "      # Makes output deterministic if graphing\n",
        "      idx = sample(prediction[-1], temperature=0)\n",
        "    else:\n",
        "      # Temperature is how variant the output will be\n",
        "      idx = sample(prediction[-1], temperature=0.7)\n",
        "    word_idxs.append(idx)\n",
        "\n",
        "  if return_probs:\n",
        "    # Get word probabilites to return as a list\n",
        "    return ' '.join(idx2word(idx) for idx in word_idxs), word_probs\n",
        "\n",
        "  return ' '.join(idx2word(idx) for idx in word_idxs)\n",
        "\n",
        "\n",
        "# Displays a sample of the output after each epoch\n",
        "def on_epoch_end(epoch, _):\n",
        "  print(f'\\nGenerating text after epoch: {epoch}')\n",
        "\n",
        "  # Sample texts to print each epoch\n",
        "  texts = [\n",
        "    'sir it is time',\n",
        "    'where are',\n",
        "    'i am a',\n",
        "    'my favorite',\n",
        "  ]\n",
        "\n",
        "  for text in texts:\n",
        "    sample = generate_next(text)\n",
        "    print(sample)\n"
      ],
      "metadata": {
        "id": "UzR53GD8Xq2C"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback function to save model\n",
        "checkpoint_path = DATA_PATH+'model_saves/cp-{epoch:04d}/model.ckpt'\n",
        "checkpoint_save = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    save_freq=50*BATCH_SIZE)"
      ],
      "metadata": {
        "id": "mZu6O3cKDR1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Model**"
      ],
      "metadata": {
        "id": "kZcERAdx-Ks-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "progress_output = keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "# Define what callbacks you want called: checkpoint_save, progress_output\n",
        "callbacks = [checkpoint_save]"
      ],
      "metadata": {
        "id": "CrUhHvC5Dnzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_x, train_y,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=1000, \n",
        "                    callbacks=callbacks)\n",
        "\n"
      ],
      "metadata": {
        "id": "BxZjJrOhZp1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting Text**"
      ],
      "metadata": {
        "id": "19WInGWdCwCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload model if needed\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Embedding layer for w2v model\n",
        "model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights]))\n",
        "\n",
        "model.add(tf.keras.layers.LSTM(units=embedding_size))\n",
        "model.add(tf.keras.layers.Dense(units=vocab_size))\n",
        "model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Change to path of model that should be loaded\n",
        "model.load_weights(DATA_PATH+'model_saves/simple_model_2/model.ckpt')"
      ],
      "metadata": {
        "id": "3pYURz2TFSSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = generate_next(\"william is\")\n",
        "\n",
        "text"
      ],
      "metadata": {
        "id": "YRrHOVotvxrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To graph confidence\n",
        "text, probs = generate_next(\"william is\", return_probs=True)"
      ],
      "metadata": {
        "id": "zp_vHVdQ_H1J"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = range(len(probs))\n",
        "y = probs\n",
        "\n",
        "text_arr = text.split()[-len(probs):]\n",
        "word_num = 0\n",
        "for lx,ly in zip(x,y):\n",
        "  \n",
        "  word = text_arr[word_num]\n",
        "\n",
        "  plt.annotate(word, (lx,ly))\n",
        "  word_num += 1\n",
        "\n",
        "plt.xticks(x)\n",
        "plt.plot(x, y)\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "IJBcmrfg09Jo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}